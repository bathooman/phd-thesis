\chapter{Introduction}
Reliable and secure communication has become a foundational requirement for every modern information system.
Software implementations that mediate communication, whether they are network stacks in operating systems or protocol libraries used in IoT devices, must operate in accordance with their specifications.
When they do not, even subtle deviations can lead to a range of problems, including interoperability failures, functional errors, and severe security vulnerabilities.
Incidents such as Heartbleed~\cite{heartbleed} and the TLS POODLE downgrade vulnerability~\cite{POODLE, poodle-bites-tls} demonstrate that even minor deviations can have catastrophic consequences.
The practical lesson is that adherence to specifications is not optional but fundamental to both security and reliability.

Testing protocol implementations is challenging for two main reasons. First, modern protocols are usually stateful and correct processing of a single packet often depends on a potentially long and intricate sequence of preceding packets.
Second, protocol messages are highly parameterized, containing numerous interdependent fields whose values in some cases influence the interpretation of other fields and the overall state of the protocol.
This creates a vast input space in which subtle combinations of parameters may expose corner cases or requirement violations that are rarely triggered in normal operation.
These characteristics make classical unit testing and ad hoc fuzzing insufficient.
Unit tests rarely explore adversarial message sequences, while naively applied fuzzing struggles to cover the complex dependencies and stateful interactions of protocol messages.
Consequently, many implementation faults remain unnoticed until they are discovered in the wild or exploited.

Several established techniques exist for systematic testing of software.
Among these, \emph{symbolic execution (SE)} has proved to be particularly effective.
SE is a white-box technique that analyzes programs in which some inputs or variables
are designated as \emph{symbolic}, and explores the code paths that are
feasible for some assignment to these symbolic inputs, thereby reasoning over the entire space of their possible values.
In the context of network protocols, this means that an implementation can be tested not only against individual packets but also virtually against entire classes of message sequences that may trigger specification violations.

This thesis investigates how SE can be adapted and extended to test the conformance of real-world network protocol implementations against their specifications.
The focus is on developing techniques and tools that make SE effective in testing the implementation of stateful, complex protocols, where both the structure of individual packets and their sequence of exchanges determine correct behavior.

Applying SE in this context first requires an understanding of how network protocols are structured and how their specifications define correct behavior.

\section{Network Protocols}
Network protocols define a set of rules that govern how distributed software and hardware systems communicate.
They specify the format and semantics of messages exchanged between entities and the order in which these messages must appear to achieve correct and reliable communication.
Protocols operate at different layers of the communication stack, from link and transport layers to application layer, and examples include TCP~\cite{RFC9293}, TLS~\cite{RFC5746}, DTLS~\cite{RFC6347}, QUIC~\cite{RFC9000}, and CoAP~\cite{RFC7252}.
Although protocols widely vary in purpose and complexity, all share the same fundamental goal of providing a well-defined interface that enables independent implementations to interoperate correctly.

Most network protocols are \emph{stateful systems}.
Each communicating entity maintains an internal state that tracks the progress of an interaction, and the processing of an incoming message depends on this state.
For example, handshake messages establish parameters such as keys, versions, or sequence numbers that determine how later messages are interpreted.

Protocol messages are also \emph{highly parameterized}.
They consist of multiple fields such as lengths, flags, versions, and sequence numbers that are often interdependent.
These dependencies determine how a message is parsed and processed.
For example, a length field defines how many bytes follow, a flag bit can change the message type, and a sequence number links the message to earlier communication.

Together, statefulness and parameterized message structures make protocol implementations particularly challenging to test, since the correctness of behavior depends both on message sequences and on complex combinations of parameter values.

Protocols may differ in purpose and environment.
An IoT device, a web server, and a client application can all implement the same protocol under different resource, performance, and operational constraints.
Despite these variations, each implementation must conform to the same rules of interaction to ensure interoperability.
These rules and expected behaviors are formally defined in protocol specifications, which are introduced in the next section.
\section{Specification vs. Implementation}
A protocol specification describes the syntax and semantics of messages, the constraints on field values, and the state transitions that occur during communication.
For internet protocols, these specifications are published as \emph{Request for Comments (RFCs)} by the Internet Engineering Task Force (IETF).
RFCs serve as the primary reference for implementers and define both mandatory and optional requirements through keywords such as \emph{MUST}, \emph{SHOULD}, and \emph{MAY}.
\section{Protocol Conformance Testing}
\section{Research Challenges}